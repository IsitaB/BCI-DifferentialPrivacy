{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8d6f4a-2859-4289-8344-c4ba3743360a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from EEGModels import EEGNet\n",
    "from mne import Epochs, pick_types\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b412f3-7760-4e20-9e7f-da14dc33312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac5eb4a-f70e-408a-bf92-a89c1325ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcf5dade-e238-405f-91cd-692cb6961d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S001/S001R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S002/S002R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S002/S002R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S002/S002R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S003/S003R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S004/S004R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S004/S004R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S004/S004R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S005/S005R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S005/S005R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S005/S005R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S006/S006R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S006/S006R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S006/S006R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S007/S007R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S007/S007R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S007/S007R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19999  =      0.000 ...   124.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S008/S008R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S008/S008R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S008/S008R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S009/S009R06.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S009/S009R10.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "Extracting EDF parameters from /Users/isita/mne_data/MNE-eegbci-data/files/eegmmidb/1.0.0/S009/S009R14.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 19679  =      0.000 ...   122.994 secs...\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Filtering raw data in 27 contiguous segments\n",
      "Setting up band-pass filter from 7 - 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 7.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 6.00 Hz)\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 265 samples (1.656 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['T0', 'feet', 'hands']\n",
      "Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.\n",
      "Not setting metadata\n",
      "405 matching events found\n",
      "No baseline correction applied\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Using data from preloaded Raw for 405 events and 801 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# # Set parameters and read data\n",
    "\n",
    "# avoid classification of evoked responses by using epochs that start 1s after\n",
    "# cue onset.\n",
    "tmin, tmax = -1.0, 4.0\n",
    "subjects = range(1,10)\n",
    "runs = [6, 10, 14]  # motor imagery: hands vs feet\n",
    "\n",
    "raw_fnames = eegbci.load_data(subjects, runs)\n",
    "raw = concatenate_raws([read_raw_edf(f, preload=True) for f in raw_fnames])\n",
    "eegbci.standardize(raw)  # set channel names\n",
    "montage = make_standard_montage(\"standard_1005\")\n",
    "raw.set_montage(montage)\n",
    "raw.annotations.rename(dict(T1=\"hands\", T2=\"feet\"))  # as documented on PhysioNet\n",
    "raw.set_eeg_reference(projection=True)\n",
    "\n",
    "# Apply band-pass filter\n",
    "raw.filter(7.0, 30.0, fir_design=\"firwin\", skip_by_annotation=\"edge\")\n",
    "\n",
    "picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude=\"bads\")\n",
    "\n",
    "# Read epochs (train will be done only between 1 and 2s)\n",
    "# Testing will be done with a running classifier\n",
    "epochs = Epochs(\n",
    "    raw,\n",
    "    event_id=[\"hands\", \"feet\"],\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    "    proj=True,\n",
    "    picks=picks,\n",
    "    baseline=None,\n",
    "    preload=True,\n",
    ")\n",
    "epochs_train = epochs.copy().crop(tmin=1.0, tmax=2.0)\n",
    "labels = epochs.events[:, -1] - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe95232b-a26d-4e10-9405-4dff0354c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract raw data. scale by 1000 due to scaling sensitivity in deep learning\n",
    "X = epochs.get_data() # format is in (trials, channels, samples)\n",
    "y = labels\n",
    "\n",
    "kernels, chans, samples = 1, 64, 801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ff3ff00-b005-4f33-8951-301e8635d9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405, 64, 801) (405,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d58a5551-39be-42c8-a272-e704330bb6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train      = X[0:224,]\n",
    "Y_train      = y[0:224]\n",
    "X_validate   = X[224:320,]\n",
    "Y_validate   = y[224:320]\n",
    "X_test       = X[320:416]\n",
    "Y_test       = y[320:416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11dc6b53-286d-4e66-b760-53ed7cdd358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (224, 64, 801, 1)\n",
      "224 train samples\n",
      "85 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train      = X_train.reshape(X_train.shape[0], chans, samples, kernels)\n",
    "X_validate   = X_validate.reshape(X_validate.shape[0], chans, samples, kernels)\n",
    "X_test       = X_test.reshape(X_test.shape[0], chans, samples, kernels)\n",
    "   \n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1a5e8bc-2ac2-4154-a290-cc78a1b8e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "class timing(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "        print(f\"Total training time: {self.total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91553f57-244b-4977-8993-32f2832d9403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69312, saving model to ./tmp/mi_checkpoint.h5\n",
      "14/14 - 1s - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6931 - val_accuracy: 0.5104 - 1s/epoch - 81ms/step\n",
      "Epoch 2/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zazu2/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.69312 to 0.69307, saving model to ./tmp/mi_checkpoint.h5\n",
      "14/14 - 1s - loss: 0.6931 - accuracy: 0.5223 - val_loss: 0.6931 - val_accuracy: 0.5104 - 958ms/epoch - 68ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.69307\n",
      "14/14 - 1s - loss: 0.6945 - accuracy: 0.4598 - val_loss: 0.6931 - val_accuracy: 0.5104 - 915ms/epoch - 65ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.69307\n",
      "14/14 - 1s - loss: 0.6933 - accuracy: 0.4821 - val_loss: 0.6931 - val_accuracy: 0.5104 - 949ms/epoch - 68ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.69307\n",
      "14/14 - 1s - loss: 0.6917 - accuracy: 0.5312 - val_loss: 0.6931 - val_accuracy: 0.5104 - 922ms/epoch - 66ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss improved from 0.69307 to 0.69305, saving model to ./tmp/mi_checkpoint.h5\n",
      "14/14 - 1s - loss: 0.6942 - accuracy: 0.4866 - val_loss: 0.6931 - val_accuracy: 0.5104 - 919ms/epoch - 66ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.69305\n",
      "14/14 - 1s - loss: 0.6922 - accuracy: 0.4955 - val_loss: 0.6931 - val_accuracy: 0.5104 - 934ms/epoch - 67ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 0.69305 to 0.69305, saving model to ./tmp/mi_checkpoint.h5\n",
      "14/14 - 1s - loss: 0.6920 - accuracy: 0.5357 - val_loss: 0.6930 - val_accuracy: 0.5104 - 974ms/epoch - 70ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.69305\n",
      "14/14 - 1s - loss: 0.6918 - accuracy: 0.5268 - val_loss: 0.6931 - val_accuracy: 0.5104 - 942ms/epoch - 67ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.69305\n",
      "14/14 - 1s - loss: 0.6936 - accuracy: 0.5179 - val_loss: 0.6931 - val_accuracy: 0.5104 - 944ms/epoch - 67ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.69305\n",
      "14/14 - 1s - loss: 0.6934 - accuracy: 0.5045 - val_loss: 0.6931 - val_accuracy: 0.5104 - 927ms/epoch - 66ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 0.69305 to 0.69299, saving model to ./tmp/mi_checkpoint.h5\n",
      "14/14 - 1s - loss: 0.6932 - accuracy: 0.4732 - val_loss: 0.6930 - val_accuracy: 0.5104 - 927ms/epoch - 66ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 0.69299 to 0.69296, saving model to ./tmp/mi_checkpoint.h5\n",
      "14/14 - 1s - loss: 0.6892 - accuracy: 0.5670 - val_loss: 0.6930 - val_accuracy: 0.5104 - 955ms/epoch - 68ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 0.69296 to 0.69295, saving model to ./tmp/mi_checkpoint.h5\n",
      "14/14 - 1s - loss: 0.6901 - accuracy: 0.5982 - val_loss: 0.6930 - val_accuracy: 0.5104 - 939ms/epoch - 67ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.69295\n",
      "14/14 - 1s - loss: 0.6877 - accuracy: 0.5223 - val_loss: 0.6930 - val_accuracy: 0.5104 - 951ms/epoch - 68ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.69295\n",
      "14/14 - 1s - loss: 0.6902 - accuracy: 0.5045 - val_loss: 0.6930 - val_accuracy: 0.5104 - 956ms/epoch - 68ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 0.69295 to 0.69294, saving model to ./tmp/mi_checkpoint.h5\n",
      "14/14 - 1s - loss: 0.6790 - accuracy: 0.5536 - val_loss: 0.6929 - val_accuracy: 0.5104 - 964ms/epoch - 69ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6758 - accuracy: 0.5893 - val_loss: 0.6931 - val_accuracy: 0.5104 - 936ms/epoch - 67ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6653 - accuracy: 0.5893 - val_loss: 0.6930 - val_accuracy: 0.5104 - 937ms/epoch - 67ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6600 - accuracy: 0.6161 - val_loss: 0.6931 - val_accuracy: 0.5312 - 923ms/epoch - 66ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6606 - accuracy: 0.6116 - val_loss: 0.6933 - val_accuracy: 0.4896 - 941ms/epoch - 67ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6486 - accuracy: 0.5938 - val_loss: 0.6937 - val_accuracy: 0.4896 - 930ms/epoch - 66ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6448 - accuracy: 0.6250 - val_loss: 0.6931 - val_accuracy: 0.5104 - 953ms/epoch - 68ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6397 - accuracy: 0.6473 - val_loss: 0.6933 - val_accuracy: 0.4896 - 937ms/epoch - 67ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6438 - accuracy: 0.6205 - val_loss: 0.6937 - val_accuracy: 0.4896 - 931ms/epoch - 66ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6462 - accuracy: 0.6384 - val_loss: 0.6937 - val_accuracy: 0.4896 - 955ms/epoch - 68ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6387 - accuracy: 0.6607 - val_loss: 0.6939 - val_accuracy: 0.4896 - 943ms/epoch - 67ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6294 - accuracy: 0.6295 - val_loss: 0.6945 - val_accuracy: 0.4896 - 940ms/epoch - 67ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6395 - accuracy: 0.6161 - val_loss: 0.6955 - val_accuracy: 0.4896 - 1s/epoch - 74ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6348 - accuracy: 0.6652 - val_loss: 0.6970 - val_accuracy: 0.4896 - 966ms/epoch - 69ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6336 - accuracy: 0.6473 - val_loss: 0.6961 - val_accuracy: 0.4896 - 950ms/epoch - 68ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6288 - accuracy: 0.6339 - val_loss: 0.6951 - val_accuracy: 0.4896 - 950ms/epoch - 68ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6162 - accuracy: 0.6741 - val_loss: 0.6956 - val_accuracy: 0.4896 - 954ms/epoch - 68ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6314 - accuracy: 0.6518 - val_loss: 0.6949 - val_accuracy: 0.4896 - 984ms/epoch - 70ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6220 - accuracy: 0.6607 - val_loss: 0.6956 - val_accuracy: 0.4896 - 961ms/epoch - 69ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6185 - accuracy: 0.6786 - val_loss: 0.6949 - val_accuracy: 0.4792 - 938ms/epoch - 67ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6149 - accuracy: 0.6875 - val_loss: 0.6954 - val_accuracy: 0.4896 - 989ms/epoch - 71ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6163 - accuracy: 0.6741 - val_loss: 0.7012 - val_accuracy: 0.4896 - 978ms/epoch - 70ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6119 - accuracy: 0.6652 - val_loss: 0.7166 - val_accuracy: 0.4896 - 958ms/epoch - 68ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6181 - accuracy: 0.7054 - val_loss: 0.7071 - val_accuracy: 0.4896 - 968ms/epoch - 69ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6272 - accuracy: 0.6429 - val_loss: 0.7105 - val_accuracy: 0.4896 - 942ms/epoch - 67ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6091 - accuracy: 0.6786 - val_loss: 0.7043 - val_accuracy: 0.5000 - 951ms/epoch - 68ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6115 - accuracy: 0.7009 - val_loss: 0.7086 - val_accuracy: 0.5000 - 949ms/epoch - 68ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6060 - accuracy: 0.7054 - val_loss: 0.7010 - val_accuracy: 0.4896 - 939ms/epoch - 67ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6091 - accuracy: 0.7009 - val_loss: 0.7040 - val_accuracy: 0.4792 - 939ms/epoch - 67ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6080 - accuracy: 0.6875 - val_loss: 0.7018 - val_accuracy: 0.4792 - 923ms/epoch - 66ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5990 - accuracy: 0.7143 - val_loss: 0.6986 - val_accuracy: 0.4792 - 1s/epoch - 72ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6098 - accuracy: 0.6786 - val_loss: 0.6958 - val_accuracy: 0.4896 - 946ms/epoch - 68ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6082 - accuracy: 0.6920 - val_loss: 0.6943 - val_accuracy: 0.5625 - 952ms/epoch - 68ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.6029 - accuracy: 0.6964 - val_loss: 0.6942 - val_accuracy: 0.5729 - 1s/epoch - 73ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5996 - accuracy: 0.7411 - val_loss: 0.6950 - val_accuracy: 0.5729 - 952ms/epoch - 68ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5890 - accuracy: 0.7902 - val_loss: 0.6979 - val_accuracy: 0.5104 - 959ms/epoch - 69ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5882 - accuracy: 0.7545 - val_loss: 0.6981 - val_accuracy: 0.5000 - 959ms/epoch - 68ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5925 - accuracy: 0.7634 - val_loss: 0.7001 - val_accuracy: 0.4896 - 923ms/epoch - 66ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5916 - accuracy: 0.7277 - val_loss: 0.6983 - val_accuracy: 0.5729 - 950ms/epoch - 68ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5974 - accuracy: 0.7321 - val_loss: 0.6988 - val_accuracy: 0.5104 - 927ms/epoch - 66ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5777 - accuracy: 0.7679 - val_loss: 0.7041 - val_accuracy: 0.5208 - 998ms/epoch - 71ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5863 - accuracy: 0.7500 - val_loss: 0.7047 - val_accuracy: 0.5208 - 940ms/epoch - 67ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5819 - accuracy: 0.7500 - val_loss: 0.7159 - val_accuracy: 0.5000 - 920ms/epoch - 66ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5777 - accuracy: 0.7545 - val_loss: 0.7070 - val_accuracy: 0.5104 - 951ms/epoch - 68ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5913 - accuracy: 0.7768 - val_loss: 0.7026 - val_accuracy: 0.5000 - 930ms/epoch - 66ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5692 - accuracy: 0.8080 - val_loss: 0.6990 - val_accuracy: 0.5104 - 926ms/epoch - 66ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5681 - accuracy: 0.7946 - val_loss: 0.6987 - val_accuracy: 0.5104 - 965ms/epoch - 69ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.69294\n",
      "14/14 - 1s - loss: 0.5763 - accuracy: 0.7723 - val_loss: 0.7008 - val_accuracy: 0.5208 - 920ms/epoch - 66ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss improved from 0.69294 to 0.69292, saving model to ./tmp/mi_checkpoint.h5\n",
      "14/14 - 1s - loss: 0.5566 - accuracy: 0.7946 - val_loss: 0.6929 - val_accuracy: 0.5938 - 937ms/epoch - 67ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5635 - accuracy: 0.8125 - val_loss: 0.7015 - val_accuracy: 0.5208 - 927ms/epoch - 66ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5597 - accuracy: 0.7857 - val_loss: 0.6997 - val_accuracy: 0.5417 - 936ms/epoch - 67ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5702 - accuracy: 0.7500 - val_loss: 0.6950 - val_accuracy: 0.5417 - 931ms/epoch - 66ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5642 - accuracy: 0.7946 - val_loss: 0.6997 - val_accuracy: 0.5312 - 955ms/epoch - 68ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5635 - accuracy: 0.7768 - val_loss: 0.7007 - val_accuracy: 0.5104 - 939ms/epoch - 67ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5502 - accuracy: 0.7634 - val_loss: 0.7034 - val_accuracy: 0.5417 - 966ms/epoch - 69ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5577 - accuracy: 0.7679 - val_loss: 0.6941 - val_accuracy: 0.5833 - 937ms/epoch - 67ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5511 - accuracy: 0.7991 - val_loss: 0.7040 - val_accuracy: 0.5208 - 935ms/epoch - 67ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5441 - accuracy: 0.8125 - val_loss: 0.7051 - val_accuracy: 0.5312 - 936ms/epoch - 67ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5337 - accuracy: 0.8482 - val_loss: 0.6966 - val_accuracy: 0.5104 - 948ms/epoch - 68ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5410 - accuracy: 0.8080 - val_loss: 0.6958 - val_accuracy: 0.5417 - 932ms/epoch - 67ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5389 - accuracy: 0.8036 - val_loss: 0.7005 - val_accuracy: 0.5312 - 921ms/epoch - 66ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5461 - accuracy: 0.7991 - val_loss: 0.7021 - val_accuracy: 0.5521 - 918ms/epoch - 66ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5379 - accuracy: 0.8170 - val_loss: 0.7048 - val_accuracy: 0.4792 - 926ms/epoch - 66ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5272 - accuracy: 0.8259 - val_loss: 0.7197 - val_accuracy: 0.4896 - 953ms/epoch - 68ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5434 - accuracy: 0.8214 - val_loss: 0.7247 - val_accuracy: 0.5104 - 936ms/epoch - 67ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5274 - accuracy: 0.8348 - val_loss: 0.7040 - val_accuracy: 0.4583 - 924ms/epoch - 66ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5294 - accuracy: 0.8438 - val_loss: 0.7167 - val_accuracy: 0.4896 - 951ms/epoch - 68ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5297 - accuracy: 0.8616 - val_loss: 0.7242 - val_accuracy: 0.4792 - 935ms/epoch - 67ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5144 - accuracy: 0.8348 - val_loss: 0.7793 - val_accuracy: 0.5000 - 1s/epoch - 73ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5243 - accuracy: 0.8348 - val_loss: 0.7505 - val_accuracy: 0.4896 - 947ms/epoch - 68ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5232 - accuracy: 0.8259 - val_loss: 0.7357 - val_accuracy: 0.4688 - 934ms/epoch - 67ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5103 - accuracy: 0.8304 - val_loss: 0.7386 - val_accuracy: 0.5000 - 925ms/epoch - 66ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5223 - accuracy: 0.8170 - val_loss: 0.7135 - val_accuracy: 0.5417 - 937ms/epoch - 67ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5096 - accuracy: 0.8393 - val_loss: 0.7049 - val_accuracy: 0.5000 - 933ms/epoch - 67ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5261 - accuracy: 0.8125 - val_loss: 0.7096 - val_accuracy: 0.5000 - 928ms/epoch - 66ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5031 - accuracy: 0.8616 - val_loss: 0.7175 - val_accuracy: 0.4896 - 934ms/epoch - 67ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5058 - accuracy: 0.8661 - val_loss: 0.7156 - val_accuracy: 0.4896 - 920ms/epoch - 66ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5171 - accuracy: 0.8170 - val_loss: 0.7100 - val_accuracy: 0.4688 - 945ms/epoch - 68ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5118 - accuracy: 0.7857 - val_loss: 0.7137 - val_accuracy: 0.5208 - 926ms/epoch - 66ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5039 - accuracy: 0.8438 - val_loss: 0.7163 - val_accuracy: 0.5208 - 996ms/epoch - 71ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5288 - accuracy: 0.7946 - val_loss: 0.7388 - val_accuracy: 0.5208 - 936ms/epoch - 67ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4970 - accuracy: 0.8393 - val_loss: 0.7216 - val_accuracy: 0.5000 - 928ms/epoch - 66ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4989 - accuracy: 0.8348 - val_loss: 0.7120 - val_accuracy: 0.5104 - 949ms/epoch - 68ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5080 - accuracy: 0.8259 - val_loss: 0.7126 - val_accuracy: 0.4896 - 953ms/epoch - 68ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4940 - accuracy: 0.8795 - val_loss: 0.7104 - val_accuracy: 0.5208 - 932ms/epoch - 67ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5069 - accuracy: 0.8259 - val_loss: 0.6987 - val_accuracy: 0.5208 - 1s/epoch - 73ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5106 - accuracy: 0.8214 - val_loss: 0.7098 - val_accuracy: 0.5000 - 1s/epoch - 72ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4830 - accuracy: 0.8661 - val_loss: 0.7190 - val_accuracy: 0.5104 - 971ms/epoch - 69ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4984 - accuracy: 0.8304 - val_loss: 0.7215 - val_accuracy: 0.4896 - 1s/epoch - 72ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4991 - accuracy: 0.8304 - val_loss: 0.7093 - val_accuracy: 0.5104 - 953ms/epoch - 68ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.5041 - accuracy: 0.8080 - val_loss: 0.7080 - val_accuracy: 0.4583 - 953ms/epoch - 68ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4847 - accuracy: 0.8705 - val_loss: 0.7107 - val_accuracy: 0.4896 - 988ms/epoch - 71ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4837 - accuracy: 0.8527 - val_loss: 0.7038 - val_accuracy: 0.4896 - 943ms/epoch - 67ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4977 - accuracy: 0.8571 - val_loss: 0.7076 - val_accuracy: 0.4896 - 924ms/epoch - 66ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4841 - accuracy: 0.8839 - val_loss: 0.7071 - val_accuracy: 0.5208 - 989ms/epoch - 71ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4855 - accuracy: 0.8527 - val_loss: 0.7030 - val_accuracy: 0.4792 - 969ms/epoch - 69ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4903 - accuracy: 0.8482 - val_loss: 0.7194 - val_accuracy: 0.5104 - 958ms/epoch - 68ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4797 - accuracy: 0.8571 - val_loss: 0.7089 - val_accuracy: 0.5104 - 954ms/epoch - 68ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4738 - accuracy: 0.8438 - val_loss: 0.7056 - val_accuracy: 0.5208 - 963ms/epoch - 69ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4682 - accuracy: 0.8661 - val_loss: 0.7236 - val_accuracy: 0.5104 - 959ms/epoch - 68ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4790 - accuracy: 0.8438 - val_loss: 0.7210 - val_accuracy: 0.5000 - 954ms/epoch - 68ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4649 - accuracy: 0.8795 - val_loss: 0.7147 - val_accuracy: 0.5521 - 941ms/epoch - 67ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4739 - accuracy: 0.8616 - val_loss: 0.7110 - val_accuracy: 0.5625 - 987ms/epoch - 70ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4702 - accuracy: 0.8705 - val_loss: 0.7179 - val_accuracy: 0.5417 - 955ms/epoch - 68ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4643 - accuracy: 0.9152 - val_loss: 0.7163 - val_accuracy: 0.5000 - 931ms/epoch - 67ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4746 - accuracy: 0.8705 - val_loss: 0.7324 - val_accuracy: 0.5208 - 972ms/epoch - 69ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4524 - accuracy: 0.8884 - val_loss: 0.7061 - val_accuracy: 0.4896 - 942ms/epoch - 67ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4533 - accuracy: 0.8973 - val_loss: 0.7257 - val_accuracy: 0.4896 - 922ms/epoch - 66ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4686 - accuracy: 0.8661 - val_loss: 0.7064 - val_accuracy: 0.4688 - 920ms/epoch - 66ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4554 - accuracy: 0.8795 - val_loss: 0.7619 - val_accuracy: 0.5208 - 923ms/epoch - 66ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4502 - accuracy: 0.8750 - val_loss: 0.7250 - val_accuracy: 0.5208 - 969ms/epoch - 69ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4494 - accuracy: 0.8661 - val_loss: 0.7170 - val_accuracy: 0.4896 - 944ms/epoch - 67ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4437 - accuracy: 0.9062 - val_loss: 0.7339 - val_accuracy: 0.5312 - 933ms/epoch - 67ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4456 - accuracy: 0.8839 - val_loss: 0.7123 - val_accuracy: 0.4896 - 927ms/epoch - 66ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4535 - accuracy: 0.8884 - val_loss: 0.7320 - val_accuracy: 0.4896 - 913ms/epoch - 65ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4543 - accuracy: 0.8929 - val_loss: 0.7164 - val_accuracy: 0.5417 - 926ms/epoch - 66ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4474 - accuracy: 0.8482 - val_loss: 0.7117 - val_accuracy: 0.5208 - 987ms/epoch - 70ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4515 - accuracy: 0.8482 - val_loss: 0.7058 - val_accuracy: 0.5312 - 941ms/epoch - 67ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4422 - accuracy: 0.8705 - val_loss: 0.7140 - val_accuracy: 0.5104 - 942ms/epoch - 67ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4481 - accuracy: 0.8482 - val_loss: 0.7231 - val_accuracy: 0.5000 - 937ms/epoch - 67ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4381 - accuracy: 0.8750 - val_loss: 0.7227 - val_accuracy: 0.5312 - 948ms/epoch - 68ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4346 - accuracy: 0.9018 - val_loss: 0.7323 - val_accuracy: 0.5208 - 936ms/epoch - 67ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4383 - accuracy: 0.9062 - val_loss: 0.7185 - val_accuracy: 0.5000 - 960ms/epoch - 69ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4424 - accuracy: 0.8750 - val_loss: 0.7284 - val_accuracy: 0.5208 - 927ms/epoch - 66ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4363 - accuracy: 0.8795 - val_loss: 0.7285 - val_accuracy: 0.4792 - 930ms/epoch - 66ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4403 - accuracy: 0.8661 - val_loss: 0.7249 - val_accuracy: 0.5104 - 930ms/epoch - 66ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4379 - accuracy: 0.8750 - val_loss: 0.7197 - val_accuracy: 0.5000 - 949ms/epoch - 68ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4330 - accuracy: 0.8795 - val_loss: 0.7322 - val_accuracy: 0.5000 - 957ms/epoch - 68ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4245 - accuracy: 0.8839 - val_loss: 0.7366 - val_accuracy: 0.5521 - 957ms/epoch - 68ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4273 - accuracy: 0.9018 - val_loss: 0.7330 - val_accuracy: 0.5104 - 963ms/epoch - 69ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4303 - accuracy: 0.8884 - val_loss: 0.7290 - val_accuracy: 0.5208 - 941ms/epoch - 67ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4144 - accuracy: 0.9420 - val_loss: 0.7409 - val_accuracy: 0.4896 - 937ms/epoch - 67ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4314 - accuracy: 0.8795 - val_loss: 0.7394 - val_accuracy: 0.5312 - 936ms/epoch - 67ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4101 - accuracy: 0.9152 - val_loss: 0.7331 - val_accuracy: 0.4896 - 921ms/epoch - 66ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4117 - accuracy: 0.9420 - val_loss: 0.7269 - val_accuracy: 0.4896 - 929ms/epoch - 66ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4083 - accuracy: 0.9330 - val_loss: 0.7469 - val_accuracy: 0.5104 - 983ms/epoch - 70ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4295 - accuracy: 0.8839 - val_loss: 0.7515 - val_accuracy: 0.5000 - 962ms/epoch - 69ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4314 - accuracy: 0.8750 - val_loss: 0.7411 - val_accuracy: 0.5208 - 953ms/epoch - 68ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4201 - accuracy: 0.8750 - val_loss: 0.7599 - val_accuracy: 0.5104 - 918ms/epoch - 66ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4339 - accuracy: 0.8839 - val_loss: 0.7463 - val_accuracy: 0.5000 - 955ms/epoch - 68ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4179 - accuracy: 0.9018 - val_loss: 0.7391 - val_accuracy: 0.5521 - 943ms/epoch - 67ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4126 - accuracy: 0.8661 - val_loss: 0.7526 - val_accuracy: 0.5000 - 946ms/epoch - 68ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4139 - accuracy: 0.9241 - val_loss: 0.7342 - val_accuracy: 0.5521 - 953ms/epoch - 68ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4031 - accuracy: 0.9152 - val_loss: 0.7504 - val_accuracy: 0.4792 - 958ms/epoch - 68ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4271 - accuracy: 0.8705 - val_loss: 0.7505 - val_accuracy: 0.5208 - 955ms/epoch - 68ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4149 - accuracy: 0.9107 - val_loss: 0.7585 - val_accuracy: 0.5104 - 948ms/epoch - 68ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4030 - accuracy: 0.9018 - val_loss: 0.7650 - val_accuracy: 0.5000 - 966ms/epoch - 69ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4265 - accuracy: 0.8750 - val_loss: 0.7414 - val_accuracy: 0.5000 - 1s/epoch - 73ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3907 - accuracy: 0.9062 - val_loss: 0.7524 - val_accuracy: 0.4896 - 968ms/epoch - 69ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3996 - accuracy: 0.8973 - val_loss: 0.7506 - val_accuracy: 0.5104 - 954ms/epoch - 68ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3954 - accuracy: 0.9107 - val_loss: 0.7617 - val_accuracy: 0.5312 - 1s/epoch - 74ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3928 - accuracy: 0.9062 - val_loss: 0.7522 - val_accuracy: 0.5417 - 1s/epoch - 74ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3952 - accuracy: 0.9107 - val_loss: 0.7519 - val_accuracy: 0.5104 - 1s/epoch - 78ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3935 - accuracy: 0.9152 - val_loss: 0.7375 - val_accuracy: 0.5208 - 982ms/epoch - 70ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.4099 - accuracy: 0.8884 - val_loss: 0.7433 - val_accuracy: 0.5208 - 981ms/epoch - 70ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3864 - accuracy: 0.9330 - val_loss: 0.7609 - val_accuracy: 0.5208 - 1s/epoch - 72ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3822 - accuracy: 0.9196 - val_loss: 0.7432 - val_accuracy: 0.5521 - 940ms/epoch - 67ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3818 - accuracy: 0.9330 - val_loss: 0.7563 - val_accuracy: 0.5104 - 1s/epoch - 73ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3739 - accuracy: 0.9286 - val_loss: 0.7360 - val_accuracy: 0.5417 - 999ms/epoch - 71ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3711 - accuracy: 0.9375 - val_loss: 0.7423 - val_accuracy: 0.5312 - 953ms/epoch - 68ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3810 - accuracy: 0.9107 - val_loss: 0.7642 - val_accuracy: 0.5417 - 1s/epoch - 74ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3819 - accuracy: 0.9107 - val_loss: 0.7673 - val_accuracy: 0.5417 - 977ms/epoch - 70ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3786 - accuracy: 0.9107 - val_loss: 0.8402 - val_accuracy: 0.4896 - 949ms/epoch - 68ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3713 - accuracy: 0.8973 - val_loss: 0.7904 - val_accuracy: 0.5104 - 965ms/epoch - 69ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3630 - accuracy: 0.9286 - val_loss: 0.8087 - val_accuracy: 0.5104 - 946ms/epoch - 68ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3880 - accuracy: 0.9107 - val_loss: 0.7751 - val_accuracy: 0.4792 - 964ms/epoch - 69ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3598 - accuracy: 0.9241 - val_loss: 0.7807 - val_accuracy: 0.4792 - 987ms/epoch - 71ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3776 - accuracy: 0.9286 - val_loss: 0.7940 - val_accuracy: 0.5000 - 1s/epoch - 77ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3702 - accuracy: 0.9286 - val_loss: 0.8948 - val_accuracy: 0.4688 - 941ms/epoch - 67ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3668 - accuracy: 0.9330 - val_loss: 0.7672 - val_accuracy: 0.5417 - 942ms/epoch - 67ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3438 - accuracy: 0.9330 - val_loss: 0.7520 - val_accuracy: 0.5104 - 1s/epoch - 77ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3566 - accuracy: 0.9375 - val_loss: 0.7447 - val_accuracy: 0.5208 - 932ms/epoch - 67ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3728 - accuracy: 0.9196 - val_loss: 0.7787 - val_accuracy: 0.5104 - 935ms/epoch - 67ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3789 - accuracy: 0.8884 - val_loss: 0.7979 - val_accuracy: 0.5312 - 922ms/epoch - 66ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3747 - accuracy: 0.9286 - val_loss: 0.7593 - val_accuracy: 0.5312 - 947ms/epoch - 68ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3720 - accuracy: 0.9196 - val_loss: 0.7863 - val_accuracy: 0.5104 - 989ms/epoch - 71ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3760 - accuracy: 0.9018 - val_loss: 0.7569 - val_accuracy: 0.5208 - 967ms/epoch - 69ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3518 - accuracy: 0.9152 - val_loss: 0.7765 - val_accuracy: 0.5312 - 1s/epoch - 77ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3690 - accuracy: 0.9107 - val_loss: 0.7746 - val_accuracy: 0.5104 - 1s/epoch - 76ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3383 - accuracy: 0.9375 - val_loss: 0.7655 - val_accuracy: 0.5208 - 951ms/epoch - 68ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3546 - accuracy: 0.9018 - val_loss: 0.7524 - val_accuracy: 0.5312 - 918ms/epoch - 66ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3519 - accuracy: 0.9420 - val_loss: 0.7829 - val_accuracy: 0.5312 - 995ms/epoch - 71ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3447 - accuracy: 0.9241 - val_loss: 0.7587 - val_accuracy: 0.5521 - 944ms/epoch - 67ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3543 - accuracy: 0.9152 - val_loss: 0.7726 - val_accuracy: 0.5521 - 937ms/epoch - 67ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3725 - accuracy: 0.9152 - val_loss: 0.7671 - val_accuracy: 0.5208 - 926ms/epoch - 66ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3431 - accuracy: 0.9375 - val_loss: 0.7808 - val_accuracy: 0.5417 - 938ms/epoch - 67ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3339 - accuracy: 0.9420 - val_loss: 0.7695 - val_accuracy: 0.5208 - 1s/epoch - 74ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3659 - accuracy: 0.9062 - val_loss: 0.8342 - val_accuracy: 0.5208 - 942ms/epoch - 67ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3449 - accuracy: 0.9241 - val_loss: 0.7570 - val_accuracy: 0.5417 - 941ms/epoch - 67ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3409 - accuracy: 0.9241 - val_loss: 0.8492 - val_accuracy: 0.5312 - 931ms/epoch - 67ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3518 - accuracy: 0.9107 - val_loss: 0.7590 - val_accuracy: 0.5312 - 941ms/epoch - 67ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3375 - accuracy: 0.9196 - val_loss: 0.7661 - val_accuracy: 0.5312 - 942ms/epoch - 67ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3311 - accuracy: 0.9509 - val_loss: 0.8001 - val_accuracy: 0.5000 - 920ms/epoch - 66ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3531 - accuracy: 0.9196 - val_loss: 0.7620 - val_accuracy: 0.5417 - 952ms/epoch - 68ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3433 - accuracy: 0.9375 - val_loss: 0.7722 - val_accuracy: 0.5104 - 984ms/epoch - 70ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3451 - accuracy: 0.9286 - val_loss: 0.7452 - val_accuracy: 0.5625 - 943ms/epoch - 67ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3409 - accuracy: 0.9107 - val_loss: 0.7487 - val_accuracy: 0.5521 - 929ms/epoch - 66ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3220 - accuracy: 0.9241 - val_loss: 0.7718 - val_accuracy: 0.5625 - 936ms/epoch - 67ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3480 - accuracy: 0.9107 - val_loss: 0.7554 - val_accuracy: 0.5729 - 948ms/epoch - 68ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3213 - accuracy: 0.9509 - val_loss: 0.7475 - val_accuracy: 0.5625 - 951ms/epoch - 68ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3490 - accuracy: 0.9018 - val_loss: 0.7660 - val_accuracy: 0.5521 - 1s/epoch - 73ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3267 - accuracy: 0.9509 - val_loss: 0.7682 - val_accuracy: 0.5625 - 947ms/epoch - 68ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3308 - accuracy: 0.9375 - val_loss: 0.7802 - val_accuracy: 0.5417 - 931ms/epoch - 66ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3367 - accuracy: 0.9509 - val_loss: 0.7556 - val_accuracy: 0.5208 - 943ms/epoch - 67ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3089 - accuracy: 0.9554 - val_loss: 0.7553 - val_accuracy: 0.5312 - 942ms/epoch - 67ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3377 - accuracy: 0.9286 - val_loss: 0.7666 - val_accuracy: 0.5208 - 936ms/epoch - 67ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3201 - accuracy: 0.9241 - val_loss: 0.7579 - val_accuracy: 0.5625 - 966ms/epoch - 69ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3214 - accuracy: 0.9554 - val_loss: 0.7684 - val_accuracy: 0.5208 - 908ms/epoch - 65ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2999 - accuracy: 0.9509 - val_loss: 0.7707 - val_accuracy: 0.5312 - 942ms/epoch - 67ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3155 - accuracy: 0.9375 - val_loss: 0.7934 - val_accuracy: 0.5312 - 909ms/epoch - 65ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3256 - accuracy: 0.9509 - val_loss: 0.7780 - val_accuracy: 0.5417 - 950ms/epoch - 68ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3095 - accuracy: 0.9286 - val_loss: 0.7579 - val_accuracy: 0.5417 - 907ms/epoch - 65ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3145 - accuracy: 0.9554 - val_loss: 0.8084 - val_accuracy: 0.5312 - 1s/epoch - 72ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3232 - accuracy: 0.9152 - val_loss: 0.7710 - val_accuracy: 0.5312 - 975ms/epoch - 70ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3084 - accuracy: 0.9554 - val_loss: 0.8175 - val_accuracy: 0.5312 - 955ms/epoch - 68ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3041 - accuracy: 0.9286 - val_loss: 0.8323 - val_accuracy: 0.5417 - 929ms/epoch - 66ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3183 - accuracy: 0.9286 - val_loss: 0.7848 - val_accuracy: 0.5521 - 1s/epoch - 76ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3281 - accuracy: 0.9330 - val_loss: 0.8358 - val_accuracy: 0.5625 - 964ms/epoch - 69ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3074 - accuracy: 0.9286 - val_loss: 0.8888 - val_accuracy: 0.5000 - 923ms/epoch - 66ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3191 - accuracy: 0.9241 - val_loss: 0.9723 - val_accuracy: 0.5208 - 952ms/epoch - 68ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3215 - accuracy: 0.9062 - val_loss: 0.7908 - val_accuracy: 0.5521 - 929ms/epoch - 66ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3318 - accuracy: 0.9241 - val_loss: 0.7722 - val_accuracy: 0.5312 - 937ms/epoch - 67ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3135 - accuracy: 0.9286 - val_loss: 0.8070 - val_accuracy: 0.5312 - 924ms/epoch - 66ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3285 - accuracy: 0.9241 - val_loss: 0.7930 - val_accuracy: 0.5000 - 950ms/epoch - 68ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3131 - accuracy: 0.9509 - val_loss: 0.7791 - val_accuracy: 0.5312 - 1s/epoch - 78ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2986 - accuracy: 0.9643 - val_loss: 0.7867 - val_accuracy: 0.5000 - 931ms/epoch - 66ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2955 - accuracy: 0.9554 - val_loss: 0.7770 - val_accuracy: 0.5417 - 941ms/epoch - 67ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2966 - accuracy: 0.9554 - val_loss: 0.8340 - val_accuracy: 0.5000 - 997ms/epoch - 71ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3023 - accuracy: 0.9375 - val_loss: 0.7801 - val_accuracy: 0.5312 - 937ms/epoch - 67ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2931 - accuracy: 0.9420 - val_loss: 0.7937 - val_accuracy: 0.5208 - 933ms/epoch - 67ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2806 - accuracy: 0.9643 - val_loss: 0.8016 - val_accuracy: 0.5208 - 935ms/epoch - 67ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2867 - accuracy: 0.9554 - val_loss: 0.8168 - val_accuracy: 0.5000 - 900ms/epoch - 64ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3029 - accuracy: 0.9554 - val_loss: 0.8218 - val_accuracy: 0.4688 - 907ms/epoch - 65ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3015 - accuracy: 0.9375 - val_loss: 0.8604 - val_accuracy: 0.5104 - 919ms/epoch - 66ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3138 - accuracy: 0.9286 - val_loss: 0.8252 - val_accuracy: 0.5000 - 913ms/epoch - 65ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2863 - accuracy: 0.9464 - val_loss: 0.7825 - val_accuracy: 0.5104 - 926ms/epoch - 66ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2857 - accuracy: 0.9554 - val_loss: 0.7818 - val_accuracy: 0.5417 - 938ms/epoch - 67ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3147 - accuracy: 0.9286 - val_loss: 0.7891 - val_accuracy: 0.5208 - 926ms/epoch - 66ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2950 - accuracy: 0.9509 - val_loss: 0.7701 - val_accuracy: 0.5417 - 977ms/epoch - 70ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3061 - accuracy: 0.9286 - val_loss: 0.7488 - val_accuracy: 0.5312 - 939ms/epoch - 67ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3061 - accuracy: 0.9241 - val_loss: 0.9302 - val_accuracy: 0.5104 - 922ms/epoch - 66ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3088 - accuracy: 0.9420 - val_loss: 0.7804 - val_accuracy: 0.5833 - 933ms/epoch - 67ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3127 - accuracy: 0.9286 - val_loss: 0.7924 - val_accuracy: 0.5208 - 946ms/epoch - 68ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3116 - accuracy: 0.9241 - val_loss: 0.7487 - val_accuracy: 0.5417 - 940ms/epoch - 67ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3051 - accuracy: 0.9420 - val_loss: 0.7754 - val_accuracy: 0.5208 - 931ms/epoch - 66ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2844 - accuracy: 0.9554 - val_loss: 0.7623 - val_accuracy: 0.5104 - 926ms/epoch - 66ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2723 - accuracy: 0.9554 - val_loss: 0.7572 - val_accuracy: 0.5417 - 927ms/epoch - 66ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.3016 - accuracy: 0.9196 - val_loss: 0.7578 - val_accuracy: 0.5521 - 947ms/epoch - 68ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2962 - accuracy: 0.9420 - val_loss: 0.7757 - val_accuracy: 0.5104 - 921ms/epoch - 66ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2647 - accuracy: 0.9732 - val_loss: 0.7751 - val_accuracy: 0.5208 - 975ms/epoch - 70ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2835 - accuracy: 0.9598 - val_loss: 0.7959 - val_accuracy: 0.5417 - 985ms/epoch - 70ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2727 - accuracy: 0.9509 - val_loss: 0.8580 - val_accuracy: 0.5521 - 970ms/epoch - 69ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2861 - accuracy: 0.9821 - val_loss: 0.8675 - val_accuracy: 0.5521 - 938ms/epoch - 67ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2815 - accuracy: 0.9420 - val_loss: 0.8511 - val_accuracy: 0.5417 - 937ms/epoch - 67ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2773 - accuracy: 0.9509 - val_loss: 0.9991 - val_accuracy: 0.5208 - 955ms/epoch - 68ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2686 - accuracy: 0.9688 - val_loss: 0.9121 - val_accuracy: 0.5417 - 933ms/epoch - 67ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2731 - accuracy: 0.9330 - val_loss: 0.8746 - val_accuracy: 0.5625 - 973ms/epoch - 70ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2771 - accuracy: 0.9509 - val_loss: 0.9436 - val_accuracy: 0.5625 - 1s/epoch - 72ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2789 - accuracy: 0.9420 - val_loss: 0.8468 - val_accuracy: 0.5312 - 932ms/epoch - 67ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2632 - accuracy: 0.9509 - val_loss: 0.9045 - val_accuracy: 0.5521 - 982ms/epoch - 70ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2728 - accuracy: 0.9330 - val_loss: 0.7739 - val_accuracy: 0.5625 - 959ms/epoch - 68ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2647 - accuracy: 0.9688 - val_loss: 0.7776 - val_accuracy: 0.5625 - 983ms/epoch - 70ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2464 - accuracy: 0.9643 - val_loss: 0.8799 - val_accuracy: 0.5521 - 910ms/epoch - 65ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2712 - accuracy: 0.9688 - val_loss: 0.8610 - val_accuracy: 0.5729 - 946ms/epoch - 68ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2659 - accuracy: 0.9598 - val_loss: 0.9357 - val_accuracy: 0.5417 - 945ms/epoch - 68ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2702 - accuracy: 0.9375 - val_loss: 0.8297 - val_accuracy: 0.5833 - 1s/epoch - 76ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2695 - accuracy: 0.9554 - val_loss: 0.8390 - val_accuracy: 0.5417 - 950ms/epoch - 68ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2491 - accuracy: 0.9643 - val_loss: 0.8549 - val_accuracy: 0.5521 - 934ms/epoch - 67ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2855 - accuracy: 0.9732 - val_loss: 0.8818 - val_accuracy: 0.5417 - 933ms/epoch - 67ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2697 - accuracy: 0.9420 - val_loss: 0.7990 - val_accuracy: 0.5729 - 928ms/epoch - 66ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2781 - accuracy: 0.9509 - val_loss: 0.8036 - val_accuracy: 0.5312 - 930ms/epoch - 66ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2583 - accuracy: 0.9598 - val_loss: 0.8245 - val_accuracy: 0.5208 - 1000ms/epoch - 71ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2505 - accuracy: 0.9598 - val_loss: 0.8689 - val_accuracy: 0.4896 - 946ms/epoch - 68ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2362 - accuracy: 0.9688 - val_loss: 0.8400 - val_accuracy: 0.5312 - 930ms/epoch - 66ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2758 - accuracy: 0.9464 - val_loss: 0.8513 - val_accuracy: 0.5208 - 930ms/epoch - 66ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2731 - accuracy: 0.9598 - val_loss: 0.8819 - val_accuracy: 0.5000 - 946ms/epoch - 68ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2758 - accuracy: 0.9554 - val_loss: 0.8498 - val_accuracy: 0.5312 - 924ms/epoch - 66ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2552 - accuracy: 0.9643 - val_loss: 0.7990 - val_accuracy: 0.5208 - 931ms/epoch - 66ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2499 - accuracy: 0.9688 - val_loss: 0.7890 - val_accuracy: 0.5104 - 944ms/epoch - 67ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2813 - accuracy: 0.9196 - val_loss: 0.7896 - val_accuracy: 0.5729 - 926ms/epoch - 66ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2484 - accuracy: 0.9732 - val_loss: 0.7972 - val_accuracy: 0.5417 - 991ms/epoch - 71ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2598 - accuracy: 0.9509 - val_loss: 0.7552 - val_accuracy: 0.5312 - 934ms/epoch - 67ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2716 - accuracy: 0.9554 - val_loss: 0.7802 - val_accuracy: 0.5208 - 955ms/epoch - 68ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.69292\n",
      "14/14 - 1s - loss: 0.2506 - accuracy: 0.9375 - val_loss: 0.8072 - val_accuracy: 0.5417 - 946ms/epoch - 68ms/step\n",
      "Total training time: 286.22 seconds\n"
     ]
    }
   ],
   "source": [
    "# configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
    "# model configurations may do better, but this is a good starting point)\n",
    "model = EEGNet(nb_classes = 2, Chans = chans, Samples = samples, \n",
    "               dropoutRate = 0.5, kernLength = 32, F1 = 8, D = 2, F2 = 16, \n",
    "               dropoutType = 'Dropout')\n",
    "\n",
    "# compile the model and set the optimizers\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# count number of parameters in the model\n",
    "numParams    = model.count_params()    \n",
    "\n",
    "# set a valid path for your system to record model checkpoints\n",
    "with h5py.File('./tmp/mi_checkpoint.h5', 'w') as f:\n",
    "    # This creates 2 attributes of arrays of different np.float types:\n",
    "    arr_in = np.array([1.0,2.0,3.0])\n",
    "    f.attrs['as_float64'] = arr_in\n",
    "    # arr_in.dtype = np.float128\n",
    "    # f.attrs['as_float128'] = arr_in\n",
    "checkpointer = ModelCheckpoint(filepath='./tmp/mi_checkpoint.h5', verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "###############################################################################\n",
    "# if the classification task was imbalanced (significantly more trials in one\n",
    "# class versus the others) you can assign a weight to each class during \n",
    "# optimization to balance it out. This data is approximately balanced so we \n",
    "# don't need to do this, but is shown here for illustration/completeness. \n",
    "###############################################################################\n",
    "\n",
    "# the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
    "# the weights all to be 1\n",
    "class_weights = {0:1, 1:1}\n",
    "\n",
    "################################################################################\n",
    "# fit the model. Due to very small sample sizes this can get\n",
    "# pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
    "# Riemannian geometry classification (below)\n",
    "################################################################################\n",
    "fittedModel = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
    "                        verbose = 2, validation_data=(X_validate, Y_validate),\n",
    "                        callbacks=[checkpointer, timing()], \n",
    "                        class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a8ed197-d171-4b05-9357-01474f307dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 12ms/step\n",
      "Classification accuracy: 0.588235 \n"
     ]
    }
   ],
   "source": [
    "# load optimal weights\n",
    "# model.load_weights('./tmp/mi_checkpoint.h5')\n",
    "\n",
    "###############################################################################\n",
    "# can alternatively used the weights provided in the repo. If so it should get\n",
    "# you 93% accuracy. Change the WEIGHTS_PATH variable to wherever it is on your\n",
    "# system.\n",
    "###############################################################################\n",
    "\n",
    "# WEIGHTS_PATH = /path/to/EEGNet-8-2-weights.h5 \n",
    "# model.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "###############################################################################\n",
    "# make prediction on test set.\n",
    "###############################################################################\n",
    "\n",
    "probs       = model.predict(X_test)\n",
    "preds       = probs.argmax(axis = -1)  \n",
    "acc         = np.mean(preds == Y_test.argmax(axis=-1))\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f1b26a9-c665-41ee-aaf0-00cb5f0c591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "############################# PyRiemann Portion ##############################\n",
    "\n",
    "# code is taken from PyRiemann's ERP sample script, which is decoding in \n",
    "# the tangent space with a logistic regression\n",
    "\n",
    "# n_components = 2  # pick some components\n",
    "\n",
    "# # set up sklearn pipeline\n",
    "# clf = make_pipeline(XdawnCovariances(n_components),\n",
    "#                     TangentSpace(metric='riemann'),\n",
    "#                     LogisticRegression())\n",
    "\n",
    "# preds_rg     = np.zeros(len(Y_test))\n",
    "\n",
    "# # reshape back to (trials, channels, samples)\n",
    "# X_train      = X_train.reshape(X_train.shape[0], chans, samples)\n",
    "# X_test       = X_test.reshape(X_test.shape[0], chans, samples)\n",
    "\n",
    "# # train a classifier with xDAWN spatial filtering + Riemannian Geometry (RG)\n",
    "# # labels need to be back in single-column format\n",
    "# clf.fit(X_train, Y_train.argmax(axis = -1))\n",
    "# preds_rg     = clf.predict(X_test)\n",
    "\n",
    "# # Printing the results\n",
    "# acc2         = np.mean(preds_rg == Y_test.argmax(axis = -1))\n",
    "# print(\"Classification accuracy: %f \" % (acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c56e01b-4c3f-48bc-8e33-9390b1268770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df8f5271-e6c4-4d37-b5d6-735dfa952af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x312604590>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the confusion matrices for both classifiers\n",
    "names        = ['hands', 'feet']\n",
    "# plt.figure(0)\n",
    "confusion_matrix_1 = confusion_matrix(preds, Y_test)\n",
    "\n",
    "# plt.figure(1)\n",
    "# confusion_matrix_2 = confusion_matrix(preds_rg, Y_test)\n",
    "\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix_1, display_labels = names)\n",
    "disp1.plot()\n",
    "\n",
    "# disp2 = ConfusionMatrixDisplay(confusion_matrix_2, display_labels = names)\n",
    "# disp2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "650b4b19-612d-4c26-b71d-4706f8407299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.52      0.57        50\n",
      "           1       0.45      0.57      0.51        35\n",
      "\n",
      "    accuracy                           0.54        85\n",
      "   macro avg       0.54      0.55      0.54        85\n",
      "weighted avg       0.56      0.54      0.54        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(preds, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9da8f6-6413-4a35-aa97-8615ca41d776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zazu2",
   "language": "python",
   "name": "zazu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
